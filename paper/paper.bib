@article{basharin,
author = {Basharin, G. P.},
title = {On a Statistical Estimate for the Entropy of a Sequence of Independent Random Variables},
journal = {Theory of Probability \& Its Applications},
volume = {4},
number = {3},
pages = {333-336},
year = {1959},
doi = {10.1137/1104033},
URL = {https://doi.org/10.1137/1104033},
eprint = {https://doi.org/10.1137/1104033},
}


@INPROCEEDINGS{bohme:fse:2020,
 author = {B{\"o}hme, Marcel and Man{\`e}s, Valentin and Cha, Sang Kil},
 title = {Boosting Fuzzer Efficiency: An Information Theoretic Perspective},
 booktitle = {Proceedings of the 14th Joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
 series = {ESEC/FSE},
 year = {2020},
 pages = {970-981},
 numpages = {12},
 doi = {10.1145/3368089.3409748}
}

@article{blackwell2023hyperfuzzing,
  title={Hyperfuzzing: black-box security hypertesting with a grey-box fuzzer},
  author={Blackwell, Daniel and Becker, Ingolf and Clark, David},
  journal={arXiv preprint arXiv:2308.09081},
  year={2023}
}

@book{MacKay2003,
  added-at = {2007-05-24T14:43:04.000+0200},
  author = {MacKay, David J. C.},
  biburl = {https://www.bibsonomy.org/bibtex/24c23fea472f6e75c0964badd83883d77/tmalsburg},
  interhash = {86f621d9d6f9f159448f768d792d4511},
  intrahash = {4c23fea472f6e75c0964badd83883d77},
  keywords = {bayesianinference book informationtheory neuralnetworks patternrecognition probabilitytheory},
  publisher = {Cambridge University Press},
  timestamp = {2007-05-24T14:43:04.000+0200},
  title = {Information Theory, Inference, and Learning Algorithms},
  doi = {10.1109/tit.2004.834752},
  year = 2003
}

@article{Rodriguez2021EntropyEst,
  title={Selecting an effective entropy estimator for short sequences of bits and bytes with maximum entropy},
  author={Contreras Rodr{\'\i}guez, Lianet and Madarro-Cap{\'o}, Evaristo Jos{\'e} and Leg{\'o}n-P{\'e}rez, Carlos Miguel and Rojas, Omar and Sosa-G{\'o}mez, Guillermo},
  journal={Entropy},
  volume={23},
  number={5},
  pages={561},
  year={2021},
  doi={10.3390/e23050561},
  publisher={MDPI}
}

@misc{grassberger2008entropy,
      title={Entropy Estimates from Insufficient Samplings},
      author={P. Grassberger},
      year={2008},
      eprint={physics/0307138},
      archivePrefix={arXiv},
      primaryClass={physics.data-an}
}

@article{chaoshen,
author = {Chao, Anne and Shen, Tsung-Jen},
year = {2003},
month = {12},
pages = {429-443},
title = {Nonparametric estimation of {S}hannon's diversity index when there are unseen species in sample. Environ Ecol Stat 10: 429-443},
volume = {10},
journal = {Environmental and Ecological Statistics},
doi = {10.1023/A:1026096204727}
}


@inproceedings{nemenman2002entropy,
  author = {Nemenman, Ilya and Shafee, Fariel and Bialek, William},
  title = {Entropy and inference, revisited},
  year = {2001},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
  pages = {471â€“478},
  numpages = {8},
  location = {Vancouver, British Columbia, Canada},
  series = {NIPS'01},
  doi={10.7551/mitpress/1120.003.0065},
}

@article{zhang,
    author = {Zhang, Zhiyi},
    title = "{Entropy Estimation in Turing's Perspective}",
    journal = {Neural Computation},
    volume = {24},
    number = {5},
    pages = {1368-1389},
    year = {2012},
    month = {05},
    abstract = "{A new nonparametric estimator of Shannon's entropy on a countable alphabet is proposed and analyzed against the well-known plug-in estimator. The proposed estimator is developed based on Turing's formula, which recovers distributional characteristics on the subset of the alphabet not covered by a size-n sample. The fundamental switch in perspective brings about substantial gain in estimation accuracy for every distribution with finite entropy. In general, a uniform variance upper bound is established for the entire class of distributions with finite entropy that decays at a rate of O(ln(n)/n) compared to O([ln(n)]2/n) for the plug-in. In a wide range of subclasses, the variance of the proposed estimator converges at a rate of O(1/n), and this rate of convergence carries over to the convergence rates in mean squared errors in many subclasses. Specifically, for any finite alphabet, the proposed estimator has a bias decaying exponentially in n. Several new bias-adjusted estimators are also discussed.}",
    issn = {0899-7667},
    doi = {10.1162/NECO_a_00266},
    url = {https://doi.org/10.1162/NECO\_a\_00266},
    eprint = {https://direct.mit.edu/neco/article-pdf/24/5/1368/865674/neco\_a\_00266.pdf},
}

@misc{hausser2009entropy,
      title={Entropy inference and the {J}ames-{S}tein estimator, with application to nonlinear gene association networks},
      author={Jean Hausser and Korbinian Strimmer},
      year={2009},
      eprint={0811.3579},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@book{Cover2006,
  added-at = {2009-04-20T21:27:16.000+0200},
  at = {2008-03-31 06:17:47},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  biburl = {https://www.bibsonomy.org/bibtex/22e9bfa879286689a14feb55b69d326c1/ywhuang},
  howpublished = {Hardcover},
  id = {1877660},
  interhash = {87ae368776946bf7a71ee476e81a2191},
  intrahash = {2e9bfa879286689a14feb55b69d326c1},
  isbn = {0471241954},
  keywords = {information-theory book},
  month = {July},
  priority = {0},
  publisher = {Wiley-Interscience},
  timestamp = {2009-04-20T21:27:16.000+0200},
  title = {Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)},
  year = 2006
}

@misc{hausser2009entropy,
      title={Entropy inference and the {J}ames-{S}tein estimator, with application to nonlinear gene association networks},
      author={Jean Hausser and Korbinian Strimmer},
      year={2009},
      eprint={0811.3579},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{pym, 
  title = {PYM},
  author = {pillowlab},
  year = 2024,
  note = {\url{https://github.com/pillowlab/PYMentropy/?tab=readme-ov-file} [Accessed: 2024-10-22]} 
} 

@misc{bub, 
  title = {BUB},
  author = {Liam Paninski},
  year = 2024,
  note = {\url{http://www.stat.columbia.edu/~liam/research/code/BUBfunc.m} [Accessed: 2024-10-22]} 
} 

@misc{nsb, 
  title = {NSB},
  author = {Ilya Nemenman},
  year = 2024,
  note = {\url{https://sourceforge.net/projects/nsb-entropy/} [Accessed: 2024-10-22]} 
} 

@misc{ndd, 
  title = {NDD},
  author = {Simone Marsili},
  year = 2024,
  note = {\url{https://github.com/simomarsili/ndd} [Accessed: 2024-10-22]} 
} 

@article{entropart,
  title = {{entropart}: An {R} Package to Measure and Partition Diversity},
  author = {Eric Marcon and Bruno H{\'e}rault},
  journal = {Journal of Statistical Software},
  year = {2015},
  volume = {67},
  number = {8},
  pages = {1--26},
  doi = {10.18637/jss.v067.i08},
}

@inproceedings{valiant,
 author = {Valiant, Paul and Valiant, Gregory},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Estimating the Unseen: Improved Estimators for Entropy and other Properties},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf},
 volume = {26},
 year = {2013}
}


