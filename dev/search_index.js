var documenterSearchIndex = {"docs":
[{"location":"data/#DataTypes","page":"Data","title":"DataTypes","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"EntropyData\nCountData\nfrom_counts\nfrom_data\nfrom_samples","category":"page"},{"location":"data/#DiscreteEntropy.EntropyData","page":"Data","title":"DiscreteEntropy.EntropyData","text":"abstract type EntropyData\nHistogram <: EntropyData\nSamples <: EntropyData\n\nIt is very easy, when confronted with a vector such as 123454 to forget whether it represents samples from a distribution, or a histogram of a (discrete) distribution. DiscreteEntropyjl attemps to make this a difficult mistake to make by enforcing a type difference between a vector of samples and a vector of counts.\n\nSee svector and cvector.\n\n\n\n\n\n","category":"type"},{"location":"data/#DiscreteEntropy.CountData","page":"Data","title":"DiscreteEntropy.CountData","text":"CountData\n\nFields\n\nmultiplicities::Matrix{Float64}  : multiplicity representation of data\nN::Float64 : total number of samples\nK::Int64   : observed support size\n\nMultiplicities\n\nAll of the estimators operate over a multiplicity representation of raw data. Raw data takes the form either of a vector of samples, or a vector of counts (ie a histogram).\n\nhistogram = [1,2,3,2,1,4]\n\nThe multiplicity representation of histogram is\n\n[1.0, 2.0) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 2\n\n[2.0, 3.0) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 2\n\n[3.0, 4.0) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 1\n\n[4.0, 5.0) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 1\n\nwhich we represent as\n\nbeginpmatrix\n4  2  3  1 \n1  2  1  2\nendpmatrix\n\nThe top row represents bin contents, and the bottom row the number of bins. We have 1 bin with a 4 elements, 2 bins with 2 elements, 1 bin with 3 elements and 2 bins with only 1 element.\n\nThe advantages of the multiplicity representation are compactness and efficiency. Instead of calculating the surprisal of a bin of 2 twice, we can calculate it once and multiply by the multiplicity. The downside of the representation may be floating point creep due to multiplication.\n\nConstructor\n\nCountData is not expected to be called directly, nor is it advised to directly manipulate the fields. Use either from_data, from_counts or from_samples instead.\n\n\n\n\n\n","category":"type"},{"location":"data/#DiscreteEntropy.from_counts","page":"Data","title":"DiscreteEntropy.from_counts","text":" from_counts(counts::AbstractVector; remove_zeros::Bool=true)\n from_counts(counts::CountVector, remove_zeros::Bool)\n\nReturn a CountData object from a vector or CountVector. Many estimators cannot handle a histogram with a 0 value bin, so there are filtered out unless remove_zeros is set to false.\n\n\n\n\n\n","category":"function"},{"location":"data/#DiscreteEntropy.from_data","page":"Data","title":"DiscreteEntropy.from_data","text":"from_data(data::AbstractVector, ::Type{T}; remove_zeros=true) where {T<:EntropyData}\n\nCreate a CountData object from a vector or matrix. The function is parameterised on whether the vector contains samples or the histogram.\n\nwhile remove_zeros defaults to true, this might not be the desired behaviour for Samples. A 0 value in the histgram causes problems for the estimators, but a 0 value in a vector of samples may be perfectly legitimate.\n\n\n\n\n\n","category":"function"},{"location":"data/#DiscreteEntropy.from_samples","page":"Data","title":"DiscreteEntropy.from_samples","text":" from_samples(sample::SampleVector, remove_zeros::Bool)\n\nReturn a CountData object from a vector of samples.\n\n\n\n\n\n","category":"function"},{"location":"data/#Vector-Types","page":"Data","title":"Vector Types","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"cvector\nsvector","category":"page"},{"location":"data/#DiscreteEntropy.cvector","page":"Data","title":"DiscreteEntropy.cvector","text":"cvector\n\n\n\n\n\n","category":"function"},{"location":"data/#DiscreteEntropy.svector","page":"Data","title":"DiscreteEntropy.svector","text":"svector\n\n\n\n\n\n","category":"function"},{"location":"estimators/#Estimators","page":"Estimators","title":"Estimators","text":"","category":"section"},{"location":"estimators/#Frequentist","page":"Estimators","title":"Frequentist","text":"","category":"section"},{"location":"estimators/","page":"Estimators","title":"Estimators","text":"maximum_likelihood\njackknife_mle\nmiller_madow\nschurmann_generalised","category":"page"},{"location":"estimators/#DiscreteEntropy.maximum_likelihood","page":"Estimators","title":"DiscreteEntropy.maximum_likelihood","text":"maximum_likelihood(data::CountData)::Float64\n\nReturn the maximum likelihood estimation of Shannon entropy of data in nats.\n\nhatH_tinyML = - sum_i=1^K p_i log(p_i)\n\nor equivalently\n\nhatH_tinyML = log(N) - frac1N sum_i=1^Kh_i log(h_i)\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.jackknife_mle","page":"Estimators","title":"DiscreteEntropy.jackknife_mle","text":"jackknife_mle(data::CountData; corrected=false)::Tuple{AbstractFloat, AbstractFloat}\n\nReturn the jackknifed estimate of data and the variance of the jackknifing (not the variance of the estimator itself).\n\nIf corrected is true, then the variance is scaled with n-1, else it is scaled with n\n\nAs found in the paper\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.miller_madow","page":"Estimators","title":"DiscreteEntropy.miller_madow","text":"miller_madow(data::CountData)\n\nReturn the Miller Madow estimation of Shannon entropy, with a positive bias based on the total number of samples seen (N) and the support size (K).\n\nhatH_tinyMM = hatH_tinyML + fracK - 12N\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.schurmann_generalised","page":"Estimators","title":"DiscreteEntropy.schurmann_generalised","text":"schurmann_generalised(data::CountVector, xis::XiVector{T}) where {T<:Real}\n\nschurmann_generalised\n\nhatH_tinySHU = psi(N) - frac1N sum_i=1^K  h_i left( psi(h_i) + (-1)^h_i _0^frac1xi_i - 1 fract^h_i-11+tdt right)\n\n\nReturn the generalised Schurmann entropy estimation, given a countvector data and a xivector xis, which must both be the same length.\n\nschurmann_generalised(data::CountVector, xis::Distribution, scalar=false)\n\nComputes the generalised Schurmann entropy estimation, given a countvector data and a distribution xis.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Overview","title":"Overview","text":"CurrentModule = DiscreteEntropy\nDocTestSetup = quote\n    using DiscreteEntropy\nend\n","category":"page"},{"location":"#index","page":"Overview","title":"DiscreteEntropy","text":"","category":"section"},{"location":"#Summary","page":"Overview","title":"Summary","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"DiscreteEntropy is a Julia package to estimate the Shannon entropy of discrete data.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"DiscreteEntropy implements a large collection of entropy estimators.","category":"page"},{"location":"#installing-DiscreteEntropy","page":"Overview","title":"Installing DiscreteEntropy","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"If you have not done so already, install Julia. Julia 1.8 and","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"higher are supported. Nightly is not (yet) supported.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Install DiscreteEntropy using","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"using Pkg; Pkg.add(\"DiscreteEntropy\")","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"or ","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"] add DiscreteEntropy","category":"page"},{"location":"#Basic-Usage","page":"Overview","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"using DiscreteEntropy\n\ndata = [1,2,3,4,3,2,1]\nh = estimate_h(from_data(data, Histogram), ChaoShen)","category":"page"}]
}
