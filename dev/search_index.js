var documenterSearchIndex = {"docs":
[{"location":"data/#DataTypes","page":"Data","title":"DataTypes","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"EntropyData\nCountData\nfrom_counts\nfrom_data\nfrom_samples","category":"page"},{"location":"data/#DiscreteEntropy.EntropyData","page":"Data","title":"DiscreteEntropy.EntropyData","text":"abstract type EntropyData\nHistogram <: EntropyData\nSamples <: EntropyData\n\nIt is very easy, when confronted with a vector such as 123454 to forget whether it represents samples from a distribution, or a histogram of a (discrete) distribution. DiscreteEntropyjl attemps to make this a difficult mistake to make by enforcing a type difference between a vector of samples and a vector of counts.\n\nSee svector and cvector.\n\n\n\n\n\n","category":"type"},{"location":"data/#DiscreteEntropy.CountData","page":"Data","title":"DiscreteEntropy.CountData","text":"CountData\n\nFields\n\nmultiplicities::Matrix{Float64}  : multiplicity representation of data\nN::Float64 : total number of samples\nK::Int64   : observed support size\n\nMultiplicities\n\nAll of the estimators operate over a multiplicity representation of raw data. Raw data takes the form either of a vector of samples, or a vector of counts (ie a histogram).\n\nhistogram = [1,2,3,2,1,4]\n\nThe multiplicity representation of histogram is\n\n[1.0, 2.0) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 2\n\n[2.0, 3.0) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 2\n\n[3.0, 4.0) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 1\n\n[4.0, 5.0) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 1\n\nwhich we represent as\n\nbeginpmatrix\n4  2  3  1 \n1  2  1  2\nendpmatrix\n\nThe top row represents bin contents, and the bottom row the number of bins. We have 1 bin with a 4 elements, 2 bins with 2 elements, 1 bin with 3 elements and 2 bins with only 1 element.\n\nThe advantages of the multiplicity representation are compactness and efficiency. Instead of calculating the surprisal of a bin of 2 twice, we can calculate it once and multiply by the multiplicity. The downside of the representation may be floating point creep due to multiplication.\n\nConstructor\n\nCountData is not expected to be called directly, nor is it advised to directly manipulate the fields. Use either from_data, from_counts or from_samples instead.\n\n\n\n\n\n","category":"type"},{"location":"data/#DiscreteEntropy.from_counts","page":"Data","title":"DiscreteEntropy.from_counts","text":" from_counts(counts::AbstractVector; remove_zeros::Bool=true)\n from_counts(counts::CountVector, remove_zeros::Bool)\n\nReturn a CountData object from a vector or CountVector. Many estimators cannot handle a histogram with a 0 value bin, so there are filtered out unless remove_zeros is set to false.\n\n\n\n\n\n","category":"function"},{"location":"data/#DiscreteEntropy.from_data","page":"Data","title":"DiscreteEntropy.from_data","text":"from_data(data::AbstractVector, ::Type{T}; remove_zeros=true) where {T<:EntropyData}\n\nCreate a CountData object from a vector or matrix. The function is parameterised on whether the vector contains samples or the histogram.\n\nwhile remove_zeros defaults to true, this might not be the desired behaviour for Samples. A 0 value in the histgram causes problems for the estimators, but a 0 value in a vector of samples may be perfectly legitimate.\n\n\n\n\n\n","category":"function"},{"location":"data/#DiscreteEntropy.from_samples","page":"Data","title":"DiscreteEntropy.from_samples","text":" from_samples(sample::SampleVector, remove_zeros::Bool)\n\nReturn a CountData object from a vector of samples.\n\n\n\n\n\n","category":"function"},{"location":"data/#Vector-Types","page":"Data","title":"Vector Types","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"cvector\nsvector","category":"page"},{"location":"data/#DiscreteEntropy.cvector","page":"Data","title":"DiscreteEntropy.cvector","text":"cvector\n\n\n\n\n\n","category":"function"},{"location":"data/#DiscreteEntropy.svector","page":"Data","title":"DiscreteEntropy.svector","text":"svector\n\n\n\n\n\n","category":"function"},{"location":"estimators/#Estimators","page":"Estimators","title":"Estimators","text":"","category":"section"},{"location":"estimators/#Frequentist","page":"Estimators","title":"Frequentist","text":"","category":"section"},{"location":"estimators/","page":"Estimators","title":"Estimators","text":"maximum_likelihood\njackknife_mle\nmiller_madow\nschurmann\nschurmann_generalised\nchao_shen\nzhang\nbonachela\nshrink\nchao_wang_jost","category":"page"},{"location":"estimators/#DiscreteEntropy.maximum_likelihood","page":"Estimators","title":"DiscreteEntropy.maximum_likelihood","text":"maximum_likelihood(data::CountData)::Float64\n\nReturn the maximum likelihood estimation of Shannon entropy of data in nats.\n\nhatH_tinyML = - sum_i=1^K p_i log(p_i)\n\nor equivalently\n\nhatH_tinyML = log(N) - frac1N sum_i=1^Kh_i log(h_i)\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.jackknife_mle","page":"Estimators","title":"DiscreteEntropy.jackknife_mle","text":"jackknife_mle(data::CountData; corrected=false)::Tuple{AbstractFloat, AbstractFloat}\n\nReturn the jackknifed estimate of data and the variance of the jackknifing (not the variance of the estimator itself).\n\nIf corrected is true, then the variance is scaled with n-1, else it is scaled with n\n\nAs found in the paper\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.miller_madow","page":"Estimators","title":"DiscreteEntropy.miller_madow","text":"miller_madow(data::CountData)\n\nReturn the Miller Madow estimation of Shannon entropy, with a positive bias based on the total number of samples seen (N) and the support size (K).\n\nhatH_tinyMM = hatH_tinyML + fracK - 12N\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.schurmann","page":"Estimators","title":"DiscreteEntropy.schurmann","text":"schurmann(data::CountData, ξ::Float64 = ℯ^(-1/2))\n\nReturn the Schurmann estimate of Shannon entropy of data in nats.\n\nhatH_SHU = psi(N) - frac1N sum_i=1^K  h_i left( psi(h_i) + (-1)^h_i _0^frac1xi - 1 fract^h_i-11+tdt right)\n\n\nThis is no one ideal value for xi, however the paper suggests e^(-12) approx 06\n\nExternal Links\n\nschurmann\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.schurmann_generalised","page":"Estimators","title":"DiscreteEntropy.schurmann_generalised","text":"schurmann_generalised(data::CountVector, xis::XiVector{T}) where {T<:Real}\n\nschurmann_generalised\n\nhatH_tinySHU = psi(N) - frac1N sum_i=1^K  h_i left( psi(h_i) + (-1)^h_i _0^frac1xi_i - 1 fract^h_i-11+tdt right)\n\n\nReturn the generalised Schurmann entropy estimation, given a countvector data and a xivector xis, which must both be the same length.\n\nschurmann_generalised(data::CountVector, xis::Distribution, scalar=false)\n\nComputes the generalised Schurmann entropy estimation, given a countvector data and a vector of xi values.\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.chao_shen","page":"Estimators","title":"DiscreteEntropy.chao_shen","text":"chao_shen(data::CountData)\n\nReturn the Chao-Shen estimate of the Shannon entropy of data in nats.\n\nhatH_CS = - sum_i=i^K frachatp_i^CS log hatp_i^CS1 - (1 - hatp_i^CS)\n\nwhere\n\nhatp_i^CS = (1 - frac1 - hatp_i^MLN) hatp_i^ML\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.zhang","page":"Estimators","title":"DiscreteEntropy.zhang","text":"zhang(data::CountData)\n\nReturn the Zhang estimate of the Shannon entropy of data in nats.\n\nThe recommended definition of Zhang's estimator is from Grabchak et al.\n\nhatH_Z = sum_i=1^K hatp_i sum_v=1^N - h_i frac1v _j=0^v-1 left( 1 + frac1 - h_iN - 1 - j right)\n\nThe actual algorithm comes from Fast Calculation of entropy with Zhang's estimator by Lozano et al..\n\nLinks\n\nEntropy estimation in turing's perspective\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.bonachela","page":"Estimators","title":"DiscreteEntropy.bonachela","text":"bonachela(data::CountData)\n\nReturn the Bonachela estimator of the Shannon entropy of data in nats.\n\nhatH_B = frac1N+2 sum_i=1^K left( (h_i + 1) sum_j=n_i + 2^N+2 frac1j right)\n\nExternal Links\n\nEntropy estimates of small data sets\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.shrink","page":"Estimators","title":"DiscreteEntropy.shrink","text":"shrink(data::CountData)\n\nReturn the Shrinkage, or James-Stein estimator of Shannon entropy for data in nats.\n\nhatH_tinySHR = - sum_i=1^K hatp_x^tinySHR log(hatp_x^tinySHR)\n\nwhere\n\nhatp_x^tinySHR = lambda t_x + (1 - lambda) hatp_x^tinyML\n\nand\n\nlambda = frac 1 - sum_x=1^K (hatp_x^tinySHR)^2(n-1) sum_x=1^K (t_x - hatp_x^tinyML)^2\n\nwith\n\nt_x = 1  K\n\nNotes\n\nBased on the implementation in the R package entropy\n\nExternal Links\n\nEntropy Inference and the James-Stein Estimator\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.chao_wang_jost","page":"Estimators","title":"DiscreteEntropy.chao_wang_jost","text":"chao_wang_jost(data::CountData)\n\nReturn the Chao Wang Jost Shannon entropy estimate of data in nats.\n\nhatH_tinyCWJ = sum_1 leq h_i leq N-1 frach_iN left(sum_k=h_i^N-1 frac1k right) +\nfracf_1N (1 - A)^-N + 1 left - log(A) - sum_r=1^N-1 frac1r (1 - A)^r right\n\nwith\n\nA = begincases\nfrac2 f_2(N-1) f_1 + 2 f_2   textif  f_2  0 \nfrac2(N-1)(f_1 - 1) + 1   textif  f_2 = 0  f_1 neq 0 \n1  textif  f_1 = f_2 = 0\nendcases\n\nwhere f_1 is the number of singletons and f_2 the number of doubletons in data.\n\nNotes\n\nThe algorithm is slightly modified port of that used in the entropart R library.\n\nExternal Links\n\nEntropy and the species accumulation curve: a novel entropy estimator via discovery rates of new species\n\n\n\n\n\n","category":"function"},{"location":"estimators/#Bayesian","page":"Estimators","title":"Bayesian","text":"","category":"section"},{"location":"estimators/","page":"Estimators","title":"Estimators","text":"bayes\njeffrey","category":"page"},{"location":"estimators/#DiscreteEntropy.bayes","page":"Estimators","title":"DiscreteEntropy.bayes","text":"bayes(data::CountData, α::AbstractFloat; K=nothing)\n\nReturns an estimate of Shannon entropy given data and a concentration parameter α.\n\nhatH_textBayes = - sum_k=1^K hatp_k^textBayes  log hatp_k^textBayes\n\nwhere\n\np_k^textBayes = fracK + αn + A\n\nand\n\nA = sum_x=1^K α_x\n\nIn addition to setting your own α, we have the following suggested choices\n\njeffrey : α = 0.5\nlaplace: α = 1.0\nschurmann_grassberger: α = 1 / K\nminimax: α = √{n} / K\n\n\n\n\n\n","category":"function"},{"location":"estimators/#DiscreteEntropy.jeffrey","page":"Estimators","title":"DiscreteEntropy.jeffrey","text":" jeffrey(data::CountData; K=nothing)\n\nCompute bayes estimate of entropy, with α = 05\n\n\n\n\n\n","category":"function"},{"location":"","page":"Overview","title":"Overview","text":"CurrentModule = DiscreteEntropy\nDocTestSetup = quote\n    using DiscreteEntropy\nend\n","category":"page"},{"location":"#index","page":"Overview","title":"DiscreteEntropy","text":"","category":"section"},{"location":"#Summary","page":"Overview","title":"Summary","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"DiscreteEntropy is a Julia package to estimate the Shannon entropy of discrete data.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"DiscreteEntropy implements a large collection of entropy estimators.","category":"page"},{"location":"#installing-DiscreteEntropy","page":"Overview","title":"Installing DiscreteEntropy","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"If you have not done so already, install Julia. Julia 1.8 and","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"higher are supported. Nightly is not (yet) supported.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Install DiscreteEntropy using","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"using Pkg; Pkg.add(\"DiscreteEntropy\")","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"or ","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"] add DiscreteEntropy","category":"page"},{"location":"#Basic-Usage","page":"Overview","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"using DiscreteEntropy\n\ndata = [1,2,3,4,3,2,1]\nh = estimate_h(from_data(data, Histogram), ChaoShen)","category":"page"}]
}
