<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Estimators · DiscreteEntropy.jl</title><meta name="title" content="Estimators · DiscreteEntropy.jl"/><meta property="og:title" content="Estimators · DiscreteEntropy.jl"/><meta property="twitter:title" content="Estimators · DiscreteEntropy.jl"/><meta name="description" content="Documentation for DiscreteEntropy.jl."/><meta property="og:description" content="Documentation for DiscreteEntropy.jl."/><meta property="twitter:description" content="Documentation for DiscreteEntropy.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">DiscreteEntropy.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../data/">Data</a></li><li><a class="tocitem" href="../est_h/">Estimate_H</a></li><li class="is-active"><a class="tocitem" href>Estimators</a><ul class="internal"><li><a class="tocitem" href="#Frequentist-Estimators"><span>Frequentist Estimators</span></a></li><li><a class="tocitem" href="#Bayesian-Estimators"><span>Bayesian Estimators</span></a></li><li><a class="tocitem" href="#Mixed-Estimators"><span>Mixed Estimators</span></a></li></ul></li><li><a class="tocitem" href="../utilities/">Utility Functions</a></li><li><a class="tocitem" href="../divergence/">Divergence and Distance</a></li><li><a class="tocitem" href="../mutual/">Mutual Information and Conditional Entropy</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Estimators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Estimators</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/kellino/DiscreteEntropy.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/kellino/DiscreteEntropy.jl/blob/main/docs/src/estimators.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Estimators"><a class="docs-heading-anchor" href="#Estimators">Estimators</a><a id="Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Estimators" title="Permalink"></a></h1><p>We split the estimators into two broad categories, which we call <em>Frequentist</em> and <em>Bayesian</em>. We also have a few composite estimators that either take an averaging or resampling approach to estimation.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.AbstractEstimator" href="#DiscreteEntropy.AbstractEstimator"><code>DiscreteEntropy.AbstractEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractEstimator</code></pre><p>Supertype for NonParameterised and Parameterised entropy estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/estimate.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.NonParameterisedEstimator" href="#DiscreteEntropy.NonParameterisedEstimator"><code>DiscreteEntropy.NonParameterisedEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NonParameterisedEstimator</code></pre><p>Type for NonParameterised  entropy estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/estimate.jl#L10-L14">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.ParameterisedEstimator" href="#DiscreteEntropy.ParameterisedEstimator"><code>DiscreteEntropy.ParameterisedEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ParameterisedEstimator</code></pre><p>Type for Parameterised  entropy estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/estimate.jl#L17-L21">source</a></section></article><h2 id="Frequentist-Estimators"><a class="docs-heading-anchor" href="#Frequentist-Estimators">Frequentist Estimators</a><a id="Frequentist-Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Frequentist-Estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.maximum_likelihood" href="#DiscreteEntropy.maximum_likelihood"><code>DiscreteEntropy.maximum_likelihood</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">maximum_likelihood(data::CountData)::Float64</code></pre><p>Compute the maximum likelihood estimation of Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{\tiny{ML}} = - \sum_{i=1}^K p_i \log(p_i)\]</p><p>or equivalently</p><p class="math-container">\[\hat{H}_{\tiny{ML}} = \log(N) - \frac{1}{N} \sum_{i=1}^{K}h_i \log(h_i)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L6-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.jackknife_mle" href="#DiscreteEntropy.jackknife_mle"><code>DiscreteEntropy.jackknife_mle</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">jackknife_mle(data::CountData; corrected=false)::Tuple{AbstractFloat, AbstractFloat}</code></pre><p>Compute the <em>jackknifed</em> <a href="#DiscreteEntropy.maximum_likelihood"><code>maximum_likelihood</code></a> estimate of data and the variance of the jackknifing (not the variance of the estimator itself).</p><p>If corrected is true, then the variance is scaled with n-1, else it is scaled with n</p><p><strong>External Links</strong></p><p><a href="https://academic.oup.com/biomet/article/65/3/625/234287">Estimation of the size of a closed population when capture probabilities vary among animals</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L29-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.miller_madow" href="#DiscreteEntropy.miller_madow"><code>DiscreteEntropy.miller_madow</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">miller_madow(data::CountData)</code></pre><p>Compute the Miller Madow estimation of Shannon entropy, with a positive bias based on the total number of samples seen (N) and the support size (K).</p><p class="math-container">\[\hat{H}_{\tiny{MM}} = \hat{H}_{\tiny{ML}} + \frac{K - 1}{2N}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L47-L56">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.grassberger" href="#DiscreteEntropy.grassberger"><code>DiscreteEntropy.grassberger</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">grassberger(data::CountData)</code></pre><p>Compute the Grassberger (1988) estimation of Shannon entropy of <code>data</code> in nats</p><p class="math-container">\[\hat{H}_{\tiny{Gr88}} = \sum_i \frac{h_i}{H} \left(\log(N) - \psi(h_i) - \frac{(-1)^{h_i}}{n_i + 1}  \right)\]</p><p>Equation 13 from <a href="https://www.academia.edu/download/47091312/0375-9601_2888_2990193-420160707-16069-1k3ppo7.pdf">Finite sample corrections to entropy and dimension estimate</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L64-L74">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.schurmann" href="#DiscreteEntropy.schurmann"><code>DiscreteEntropy.schurmann</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">schurmann(data::CountData, ξ::Float64 = ℯ^(-1/2))</code></pre><p>Compute the Schurmann estimate of Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{SHU} = \psi(N) - \frac{1}{N} \sum_{i=1}^{K} \, h_i \left( \psi(h_i) + (-1)^{h_i} ∫_0^{\frac{1}{\xi} - 1} \frac{t^{h_i}-1}{1+t}dt \right)
\]</p><p>This is no one ideal value for <span>$\xi$</span>, however the paper suggests <span>$e^{(-1/2)} \approx 0.6$</span></p><p><strong>External Links</strong></p><p><a href="https://arxiv.org/pdf/cond-mat/0403192.pdf">schurmann</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L87-L100">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.schurmann_generalised" href="#DiscreteEntropy.schurmann_generalised"><code>DiscreteEntropy.schurmann_generalised</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">schurmann_generalised(data::CountVector, xis::XiVector{T}) where {T&lt;:Real}</code></pre><p><a href="https://arxiv.org/pdf/2111.11175.pdf">schurmann_generalised</a></p><p class="math-container">\[\hat{H}_{\tiny{SHU}} = \psi(N) - \frac{1}{N} \sum_{i=1}^{K} \, h_i \left( \psi(h_i) + (-1)^{h_i} ∫_0^{\frac{1}{\xi_i} - 1} \frac{t^{h_i}-1}{1+t}dt \right)
\]</p><p>Compute the generalised Schurmann entropy estimation, given a countvector <code>data</code> and a xivector <code>xis</code>, which must both be the same length.</p><pre><code class="nohighlight hljs">schurmann_generalised(data::CountVector, xis::Distribution, scalar=false)</code></pre><p>Computes the generalised Schurmann entropy estimation, given a countvector <em>data</em> and a vector of <em>xi</em> values.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L115-L132">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.bub" href="#DiscreteEntropy.bub"><code>DiscreteEntropy.bub</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> bub(data::CountData; k_max=11, truncate=false, lambda=0.0)</code></pre><p>Compute The Best Upper Bound (BUB) estimation of Shannon entropy.</p><p><strong>Example</strong></p><pre><code class="language- hljs">n = [1,2,3,4,5,4,3,2,1]
(h, MM) = bub(from_counts(n))
(2.475817360451392, 0.6542542616181388)</code></pre><p>where h is the estimation of Shannon entropy in <code>nats</code> and MM is the upper bound on rms error</p><p><strong>External Links</strong></p><p><a href="https://watermark.silverchair.com/089976603321780272.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA1wwggNYBgkqhkiG9w0BBwagggNJMIIDRQIBADCCAz4GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM0ehw1kCaNd562GwpAgEQgIIDD_n4g4aLZda9boTVZsgQpyahxo-C4fFpEfzOUj_-iZ1gJh12HjkrpqIqrcBW6r18YwtMei4RRCZu90KuoxLqfvH7P5vdOVfYM-TsChkmzKWl5ajUZADxogTOqKFYMWjovXlOTJMvj8Nj484WOxceGzLwxs-xgE5qawaIpeKR4qqIR_AhmGpKrogKHJ7SHy2feKSCzpw0mdZcAV1BFmSXPiJe4djQ3kGlEEuQLVCXfV3ZzasGUrZ-45rPBsLfn9OX2-qDnGtsmmpvt8AuMe7znggoFMTUQE1B4JL79auno5RUSfu7bXGA0vBOsBopTo86c2ENkayliPTFXDExVp_QB75zmLyBtNKE0it8brZr4HAyDfGQyY956x8a6xtVS1vtZrio9AoG3Jfl52m8heXmvsQr4M51YIBMKx5bP0ext9uaBAMwwW3XEIKlIl22EI50iKsGVV-N8LWCuyrLR9LAAo-raJAOn1mbg268rtvpfGA9_sqHRc7Anal8YgJABRL_b6f7_xzT2tclyaRa60l9-9m3l2WtQpYd7UyrhPrN5-7IYGaGyKZbek4mDys4KwyqIRZDkpcgxSuHxXUZO7jbu1e5ek9Tg4RAGSAz1901aPj6PsF3ttsIeosrfAkK4c8xyrjHBIkxZO-4zBurhgDjGh3Yeo787FaN4j1bdsfTFLUC7cXghxkDMQzO4l_gunR1J7PVnRbHGqdkvZwTveP_xDRRex-D_y2jJ3r7cemZI-WmjT-NIJSflYyI9KUgqQ6y_6rTE696ttVkbrgJ1Sh95J_ISQvLzM4AjUDwCjFqpZxGvqTmK3B4MRbWlriC3QVF_wMfW5-CM2robw3n7HjlEpHDU85k5CYvPrvZG32OVU5Y9wI_PZTe94o2KuoYaC9cShqyk90lZmSN4gBz3bMgyWpGwZLy1-U84xpMphAzUHqsDV4wNQwJhbSRSO6d7G07DrfBm5yQnUXatJOTNZlZrM9UWu5e2pFCVjt79onv1TYbYA6USTsoewkHvlOaOFrXs2P07ESqLC1CIA2HcDYEkw">Estimation of Entropy and Mutual Information</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/bub.jl#L6-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.chao_shen" href="#DiscreteEntropy.chao_shen"><code>DiscreteEntropy.chao_shen</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">chao_shen(data::CountData)</code></pre><p>Compute the Chao-Shen estimate of the Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{CS} = - \sum_{i=i}^{K} \frac{\hat{p}_i^{CS} \log \hat{p}_i^{CS}}{1 - (1 - \hat{p}_i^{CS})}\]</p><p>where</p><p class="math-container">\[\hat{p}_i^{CS} = (1 - \frac{1 - \hat{p}_i^{ML}}{N}) \hat{p}_i^{ML}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L162-L175">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.zhang" href="#DiscreteEntropy.zhang"><code>DiscreteEntropy.zhang</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">zhang(data::CountData)</code></pre><p>Compute the Zhang estimate of the Shannon entropy of <code>data</code> in nats.</p><p>The recommended definition of Zhang&#39;s estimator is from <a href="https://www.tandfonline.com/doi/full/10.1080/09296174.2013.830551">Grabchak <em>et al.</em></a></p><p class="math-container">\[\hat{H}_Z = \sum_{i=1}^K \hat{p}_i \sum_{v=1}^{N - h_i} \frac{1}{v} ∏_{j=0}^{v-1} \left( 1 + \frac{1 - h_i}{N - 1 - j} \right)\]</p><p>The actual algorithm comes from <a href="https://arxiv.org/abs/1707.08290">Fast Calculation of entropy with Zhang&#39;s estimator</a> by Lozano <em>et al.</em>.</p><p><strong>Exernal Links</strong></p><p><a href="https://dl.acm.org/doi/10.1162/NECO_a_00266">Entropy estimation in turing&#39;s perspective</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L198-L212">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.bonachela" href="#DiscreteEntropy.bonachela"><code>DiscreteEntropy.bonachela</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bonachela(data::CountData)</code></pre><p>Compute the Bonachela estimator of the Shannon entropy of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{B} = \frac{1}{N+2} \sum_{i=1}^{K} \left( (h_i + 1) \sum_{j=n_i + 2}^{N+2} \frac{1}{j} \right)\]</p><p><strong>External Links</strong></p><p><a href="https://arxiv.org/pdf/0804.4561.pdf">Entropy estimates of small data sets</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L230-L241">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.shrink" href="#DiscreteEntropy.shrink"><code>DiscreteEntropy.shrink</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">shrink(data::CountData)</code></pre><p>Compute the Shrinkage, or James-Stein estimator of Shannon entropy for <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{\tiny{SHR}} = - \sum_{i=1}^{K} \hat{p}_x^{\tiny{SHR}} \log(\hat{p}_x^{\tiny{SHR}})\]</p><p>where</p><p class="math-container">\[\hat{p}_x^{\tiny{SHR}} = \lambda t_x + (1 - \lambda) \hat{p}_x^{\tiny{ML}}\]</p><p>and</p><p class="math-container">\[\lambda = \frac{ 1 - \sum_{x=1}^{K} (\hat{p}_x^{\tiny{SHR}})^2}{(n-1) \sum_{x=1}^K (t_x - \hat{p}_x^{\tiny{ML}})^2}\]</p><p>with</p><p class="math-container">\[t_x = 1 / K\]</p><p><strong>Notes</strong></p><p>Based on the implementation in the R package <a href="https://cran.r-project.org/web/packages/entropy/index.html">entropy</a></p><p><strong>External Links</strong></p><p><a href="https://www.jmlr.org/papers/volume10/hausser09a/hausser09a.pdf">Entropy Inference and the James-Stein Estimator</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L258-L287">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.chao_wang_jost" href="#DiscreteEntropy.chao_wang_jost"><code>DiscreteEntropy.chao_wang_jost</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">chao_wang_jost(data::CountData)</code></pre><p>Compute the Chao Wang Jost Shannon entropy estimate of <code>data</code> in nats.</p><p class="math-container">\[\hat{H}_{\tiny{CWJ}} = \sum_{1 \leq h_i \leq N-1} \frac{h_i}{N} \left(\sum_{k=h_i}^{N-1} \frac{1}{k} \right) +
\frac{f_1}{N} (1 - A)^{-N + 1} \left\{ - \log(A) - \sum_{r=1}^{N-1} \frac{1}{r} (1 - A)^r \right\}\]</p><p>with</p><p class="math-container">\[A = \begin{cases}
\frac{2 f_2}{(N-1) f_1 + 2 f_2} \, &amp; \text{if} \, f_2 &gt; 0 \\
\frac{2}{(N-1)(f_1 - 1) + 1} \, &amp; \text{if} \, f_2 = 0, \; f_1 \neq 0 \\
1, &amp; \text{if} \, f_1 = f_2 = 0
\end{cases}\]</p><p>where <span>$f_1$</span> is the number of singletons and <span>$f_2$</span> the number of doubletons in <code>data</code>.</p><p><strong>Notes</strong></p><p>The algorithm is slightly modified port of that used in the <a href="https://github.com/EricMarcon/entropart/blob/master/R/Shannon.R">entropart</a> R library.</p><p><strong>External Links</strong></p><p><a href="https://chao.stat.nthu.edu.tw/wordpress/paper/99.pdf">Entropy and the species accumulation curve: a novel entropy estimator via discovery rates of new species</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/frequentist.jl#L329-L357">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.unseen" href="#DiscreteEntropy.unseen"><code>DiscreteEntropy.unseen</code></a> — <span class="docstring-category">Function</span></header><section><div><p>unseen(data::CountData)</p><p>Compute the Unseen estimatation of Shannon entropy.</p><p><strong>Example</strong></p><p>``@jldoctest n = [1,2,3,4,5,4,3,2,1] h = unseen(from_counts(n)) 1.4748194918254784</p><p><strong>External Links</strong></p><p><a href="https://drive.google.com/file/d/1mdmbAZm22uH-Shr18YQTtKeMnlwypnpp/view">Estimating the Unseen: Improved Estimators for Entropy and Other Properties</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Frequentist/unseen.jl#L20-L34">source</a></section></article><h2 id="Bayesian-Estimators"><a class="docs-heading-anchor" href="#Bayesian-Estimators">Bayesian Estimators</a><a id="Bayesian-Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.bayes" href="#DiscreteEntropy.bayes"><code>DiscreteEntropy.bayes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bayes(data::CountData, α::AbstractFloat; K=nothing)</code></pre><p>Compute an estimate of Shannon entropy given data and a concentration parameter <span>$α$</span>. If K is not provided, then the observed support size in <code>data</code> is used.</p><p class="math-container">\[\hat{H}_{\text{Bayes}} = - \sum_{k=1}^{K} \hat{p}_k^{\text{Bayes}} \; \log \hat{p}_k^{\text{Bayes}}\]</p><p>where</p><p class="math-container">\[p_k^{\text{Bayes}} = \frac{K + α}{n + A}\]</p><p>and</p><p class="math-container">\[A = \sum_{x=1}^{K} α_{x}\]</p><p>In addition to setting your own α, we have the following suggested choices</p><ol><li><a href="https://ieeexplore.ieee.org/document/1056331">jeffrey</a> : α = 0.5</li><li>laplace: α = 1.0</li><li>schurmann_grassberger: α = 1 / K</li><li>minimax: α = √{n} / K</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Bayesian/bayesian.jl#L4-L31">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.jeffrey" href="#DiscreteEntropy.jeffrey"><code>DiscreteEntropy.jeffrey</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> jeffrey(data::CountData; K=nothing)</code></pre><p>Compute <a href="#DiscreteEntropy.bayes"><code>bayes</code></a> estimate of entropy, with <span>$α = 0.5$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Bayesian/bayesian.jl#L41-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.laplace" href="#DiscreteEntropy.laplace"><code>DiscreteEntropy.laplace</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> laplace(data::CountData; K=nothing)</code></pre><p>Compute <a href="#DiscreteEntropy.bayes"><code>bayes</code></a> estimate of entropy, with <span>$α = 1.0$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Bayesian/bayesian.jl#L51-L56">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.schurmann_grassberger" href="#DiscreteEntropy.schurmann_grassberger"><code>DiscreteEntropy.schurmann_grassberger</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> schurmann_grassberger(data::CountData; K=nothing)</code></pre><p>Compute <a href="#DiscreteEntropy.bayes"><code>bayes</code></a> estimate of entropy, with <span>$α = \frac{1}{K}$</span>. If K is nothing, then use data.K</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Bayesian/bayesian.jl#L61-L66">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.minimax" href="#DiscreteEntropy.minimax"><code>DiscreteEntropy.minimax</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> minimax(data::CountData; K=nothing)</code></pre><p>Compute <a href="#DiscreteEntropy.bayes"><code>bayes</code></a> estimate of entropy, with <span>$α = √\frac{data.N}$</span> where K = data.K if K is nothing.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Bayesian/bayesian.jl#L74-L79">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.nsb" href="#DiscreteEntropy.nsb"><code>DiscreteEntropy.nsb</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">nsb(data, K=data.K)</code></pre><p>Returns the Bayesian estimate of Shannon entropy of data, using the Nemenman, Shafee, Bialek algorithm</p><p class="math-container">\[\hat{H}^{\text{NSB}} = \frac{ \int_0^{\ln(K)} d\xi \, \rho(\xi, \textbf{n}) \langle H^m \rangle_{\beta (\xi)}  }
                            { \int_0^{\ln(K)} d\xi \, \rho(\xi\mid n)}\]</p><p>where</p><p class="math-container">\[\rho(\xi \mid \textbf{n}) =
    \mathcal{P}(\beta (\xi)) \frac{ \Gamma(\kappa(\xi))}{\Gamma(N + \kappa(\xi))}
    \prod_{i=1}^K \frac{\Gamma(n_i + \beta(\xi))}{\Gamma(\beta(\xi))}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Bayesian/nsb.jl#L85-L102">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.ansb" href="#DiscreteEntropy.ansb"><code>DiscreteEntropy.ansb</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ansb(data::CountData; undersampled::Float64=0.1)::Float64</code></pre><p>Return the Asymptotic NSB estimation of the Shannon entropy of <code>data</code> in nats.</p><p>See <a href="https://arxiv.org/pdf/physics/0306063.pdf">Asymptotic NSB estimator</a> (equations 11 and 12)</p><p class="math-container">\[\hat{H}_{\tiny{ANSB}} = (C_\gamma - \log(2)) + 2 \log(N) - \psi(\Delta)\]</p><p>where <span>$C_\gamma$</span> is Euler&#39;s Gamma (<span>$\approx 0.57721...$</span>), <span>$\psi_0$</span> is the digamma function and <span>$\Delta$</span> the number of coincidences in the data.</p><p>This is designed for the extremely undersampled regime (K ~ N) and diverges with N when well-sampled. ANSB requires that <span>$N/K → 0$</span>, which we set to be <span>$N/K &lt; 0.1$</span> by default</p><p><strong>External Links</strong></p><p><a href="https://arxiv.org/pdf/physics/0306063.pdf">Asymptotic NSB estimator</a> (equations 11 and 12)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Bayesian/nsb.jl#L8-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.pym" href="#DiscreteEntropy.pym"><code>DiscreteEntropy.pym</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">pym(mm::Vector{Int64}, icts::Vector{Int64})::Float64</code></pre><p>A more or less faithful port of the original <a href="https://github.com/pillowlab/PYMentropy">matlab code</a> to Julia</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/Bayesian/pym.jl#L343-L349">source</a></section></article><h2 id="Mixed-Estimators"><a class="docs-heading-anchor" href="#Mixed-Estimators">Mixed Estimators</a><a id="Mixed-Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Mixed-Estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.pert" href="#DiscreteEntropy.pert"><code>DiscreteEntropy.pert</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">pert(data::CountData, estimator::Type{T}) where {T&lt;:AbstractEstimator}
pert(data::CountData, e1::Type{T}, e2::Type{T}) where {T&lt;:AbstractEstimator}</code></pre><p>A Pert estimate of entropy, where</p><pre><code class="nohighlight hljs">a = best estimate
b = most likely estimate
c = worst case estimate</code></pre><pre><code class="nohighlight hljs">H = \frac{a + 4b + c}{6}</code></pre><p>where the default estimators are: a = maximum_likelihood, c = ANSB and <span>$b$</span> is the most likely value = ChaoShen</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/estimate.jl#L208-L225">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.jackknife" href="#DiscreteEntropy.jackknife"><code>DiscreteEntropy.jackknife</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> jackknife(data::CountData, estimator::Type{T}; corrected=false) where {T&lt;:AbstractEstimator}</code></pre><p>Compute the jackknifed estimate of <code>estimator</code> on <code>data</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/resample.jl#L65-L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DiscreteEntropy.bayesian_bootstrap" href="#DiscreteEntropy.bayesian_bootstrap"><code>DiscreteEntropy.bayesian_bootstrap</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> bayesian_bootstrap(samples::SampleVector, estimator::Type{T}, reps, seed, concentration) where {T&lt;:AbstractEstimator}</code></pre><p>Compute a bayesian bootstrap resampling of <code>samples</code> for estimation with <code>estimator</code>, where <code>reps</code> is number of resampling to perform, seed is the random seed and concentration is the concentration parameter for a Dirichlet distribution.</p><p><strong>Note</strong></p><p>Based on this [link](https://towardsdatascience.com/the-bayesian-bootstrap-6ca4a1d45148</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kellino/DiscreteEntropy.jl/blob/4fe8ea41fab0cfe3239348429841d73b93977e45/src/Estimators/resample.jl#L89-L98">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../est_h/">« Estimate_H</a><a class="docs-footer-nextpage" href="../utilities/">Utility Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Monday 16 September 2024 13:10">Monday 16 September 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
